from dataclasses import dataclass
from typing import List, Union

from core.llms.online_llms import LLMsResponse
from core.utils.utils import extract_boxed_from_str, extract_last_code_block
from core.utils.python_executor import PythonExecutor

@dataclass
class InferenceResult:
    """
    Class to store inference results.
    """
    prompt: Union[str, List[str]]
    response: Union[LLMsResponse, List[LLMsResponse]]
    answer: Union[str, List[str]]

def vanilla(type_prompt, batch_dataset, temperature, max_new_tokens, llm, args) -> List[InferenceResult]:
    """
    Vanilla inference function.
    """
    prompts = [type_prompt + f"\nQuestion: {q}\nAnswer: " for q in batch_dataset['question']]
    responses = llm.completion_batch(
                prompts=prompts,
                temperature=temperature,
                max_tokens=max_new_tokens,
                stop_tokens=["Question:"]

            )
    infer_results = []

    # Format the results
    for response, prompt in zip(responses, prompts):
        if response.error:
            raise ValueError(f"LLM error: {response.error}")
        infer_results.append(
            InferenceResult(
                prompt=prompt,
                response=response,
                answer=response.response.strip() # Different from the prompt, this is the answer generated by the LLM
            )
        )
    return infer_results

def cot(type_prompt, batch_dataset, temperature, max_new_tokens, llm, args) -> List[InferenceResult]:
    """
    Chain of Thought (CoT) inference function.
    """
    prompts = [type_prompt + f"\nQuestion: {q}\nAnswer: " for q in batch_dataset['question']]
    responses = llm.completion_batch(
                prompts=prompts,
                temperature=temperature,
                max_tokens=max_new_tokens,
                stop_tokens=["Question:"]
            )
    infer_results = []

    # Format the results
    for response, prompt in zip(responses, prompts):
        if response.error:
            raise ValueError(f"LLM error: {response.error}")
        infer_results.append(
            InferenceResult(
                prompt=prompt,
                response=response,
                answer=extract_boxed_from_str(response.response.strip()), # Extract the boxed answer from the response
            )
        )
    return infer_results

def pal(type_prompt, batch_dataset, temperature, max_new_tokens, llm, args) -> List[InferenceResult]:
    """
    PAL inference function.
    """
    prompts = [type_prompt + f"\nQuestion: {q}\nAnswer: " for q in batch_dataset['question']]
    responses = llm.completion_batch(
                prompts=prompts,
                temperature=temperature,
                max_tokens=max_new_tokens,
                stop_tokens=["Question:"], # Stop tokens for code blocks
            )
    python_executor = PythonExecutor()

    code_blocks = []
    for response in responses:
        if response.error:
            raise ValueError(f"LLM error: {response.error}")
        code_blocks.append(extract_last_code_block(response.response.strip()))

    # Execute the code blocks and get the results
    exec_results = python_executor.batch_execute_codes(code_blocks)
    infer_results = []
    # Format the results
    for response, prompt, exec_result in zip(responses, prompts, exec_results):
        if response.error:
            raise ValueError(f"LLM error: {response.error}")
        infer_results.append(
            InferenceResult(
                prompt=prompt,
                response=response,
                answer=exec_result.output # Extract the boxed answer from the response
            )
        )
    return infer_results